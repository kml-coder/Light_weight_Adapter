{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bf5bb4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import kss\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertModel\n",
    "\n",
    "# 현재 적정 dp alignemtn 하이파라미터\n",
    "# def paragraph_dp_align_v2(eng_pars, kor_pars, debug = False, max_merge=3, skip_penalty=0.35, method=\"mean\", merge_threshold = 0.45, match_threshold=0.6, merge_bonus=0.08):\n",
    "\n",
    "#TODO\n",
    "#나중에 더 적정값 찾기, 아마 \n",
    "\n",
    "\n",
    "\n",
    "# --- 0. 설정 및 파일 경로 ---\n",
    "EMBEDDING_DIM = 768\n",
    "LABSE = \"setu4993/LaBSE\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_SENT = 200  \n",
    "\n",
    "sent_tags = []\n",
    "for i in range(MAX_SENT):\n",
    "    sent_tags.append(f\"[S{i}]\")\n",
    "    sent_tags.append(f\"[/S{i}]\")\n",
    "\n",
    "\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "xlmr_model = AutoModel.from_pretrained(\"xlm-roberta-large\").to(DEVICE)\n",
    "xlmr_model.eval()\n",
    "\n",
    "kobert_tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "kobert_model = BertModel.from_pretrained(\"skt/kobert-base-v1\").to(DEVICE)\n",
    "kobert_model.eval()\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "bert_model.eval()\n",
    "\n",
    "labase_tokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")\n",
    "labase_model = AutoModel.from_pretrained(\"setu4993/LaBSE\").to(DEVICE)\n",
    "labase_model.eval()\n",
    "\n",
    "def save_sentences(sent_list, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in sent_list:\n",
    "            f.write(s.strip() + \"\\n\")\n",
    "\n",
    "\n",
    "# Load Text\n",
    "def load_and_segment_text(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 파일 없음 -> {file_path}\")\n",
    "        return \"\" \n",
    "    \n",
    "def clean_non_text_lines(text):\n",
    "    clean = []\n",
    "    for line in text.splitlines():\n",
    "        # 영문, 한글,숫자 중 하나라도 포함하면 정상\n",
    "        if re.search(r'[A-Za-z0-9가-힣]', line):\n",
    "            clean.append(line)\n",
    "        # 아니면 문자가 없는 장식이면 skip\n",
    "    return \"\\n\".join(clean)\n",
    "\n",
    "\n",
    "\n",
    "# Split sentence\n",
    "\n",
    "def split_paragraphs(raw_text):\n",
    "    raw_paragraphs = re.split(r'\\n\\s*\\n+', raw_text)\n",
    "    paragraphs = [\n",
    "        re.sub(r'\\n+', ' ', p).strip()\n",
    "        for p in raw_paragraphs \n",
    "        if (stripped := p.strip()) and re.search(r'[A-Za-z0-9가-힣]', stripped)\n",
    "    ]\n",
    "    return paragraphs\n",
    "\n",
    "def split_by_punctuation(text):\n",
    "    parts = re.split(r'(?<=[.!?])\\s+(?=[A-Za-z0-9“\"‘\\'가-힣])', text)\n",
    "    return parts\n",
    "\n",
    "def split_sentences(text):\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    quote_blocks = re.split(r'(“[^”]+”|\"[^\"]+\")', text)\n",
    "\n",
    "    for block in quote_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "\n",
    "        if (block.startswith(\"“\") and block.endswith(\"”\")) or \\\n",
    "           (block.startswith('\"') and block.endswith('\"')):\n",
    "            sentences.extend(split_by_punctuation(block))\n",
    "            continue\n",
    "\n",
    "        paren_blocks = re.split(r'(\\([^()]+\\)[.!?;,]?)', block)\n",
    "\n",
    "        for pblock in paren_blocks:\n",
    "            if not pblock.strip():\n",
    "                continue\n",
    "\n",
    "            if pblock.startswith(\"(\") and pblock.endswith(\")\"):\n",
    "                sentences.extend(split_by_punctuation(pblock))\n",
    "            else:\n",
    "                sentences.extend(split_by_punctuation(pblock))\n",
    "\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "# functions\n",
    "\n",
    "def get_embeddings_cached(sentences):\n",
    "    inputs = labase_tokenizer(\n",
    "        sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = labase_model(**inputs)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return cls_emb.cpu().numpy()\n",
    "\n",
    "def merge_embeddings(emb_list, method=\"mean\"):\n",
    "    emb = np.stack(emb_list, axis=0)\n",
    "    if method == \"mean\":\n",
    "        return emb.mean(axis=0)\n",
    "    else:  # sum 방식\n",
    "        return emb.sum(axis=0)\n",
    "    \n",
    "# DP aligner\n",
    "\n",
    "def get_paragraph_embedding_xlmr(paragraph, tokenizer, model,\n",
    "                                 layer_indices=[8,10,12]):\n",
    "    encoded = tokenizer(\n",
    "        paragraph,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    encoded = {k:v.to(DEVICE) for k,v in encoded.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = out.hidden_states\n",
    "    pooled = []\n",
    "\n",
    "    for li in layer_indices:\n",
    "        h = hidden_states[li][0]  \n",
    "        pooled.append(h.mean(dim=0).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(pooled, axis=0)   \n",
    "\n",
    "#TODO\n",
    "# N:M 집어넣기\n",
    "\n",
    "def paragraph_dp_align_v2(eng_pars, kor_pars, debug = False, max_merge=3, skip_penalty=0.7, method=\"mean\", k_bonus = 0.0, e_bonus =0.0):\n",
    "\n",
    "    N = len(eng_pars)\n",
    "    M = len(kor_pars)\n",
    "\n",
    "    # Fast embedding\n",
    "    eng_emb = get_embeddings_cached(eng_pars)\n",
    "    kor_emb = get_embeddings_cached(kor_pars)\n",
    "\n",
    "    dp = np.zeros((N+1, M+1))\n",
    "    ptr = [[None] * (M+1) for _ in range(N+1)]\n",
    "\n",
    "    sim_11 = cosine_similarity(eng_emb, kor_emb)\n",
    "\n",
    "    for j in range(1, M+1):\n",
    "        dp[0][j] = dp[0][j-1] - skip_penalty\n",
    "        ptr[0][j] = (0, j-1, 0, 1) # skip KOR\n",
    "\n",
    "    for i in range(1, N+1):\n",
    "        dp[i][0] = dp[i-1][0] - skip_penalty\n",
    "        ptr[i][0] = (i-1, 0, 1, 0) # skip ENG\n",
    "\n",
    "    for i in range(1, N+1):\n",
    "        for j in range(1, M+1):\n",
    "\n",
    "            best_score = -1e15\n",
    "            best_ptr = None\n",
    "\n",
    "            # 1:1 match\n",
    "            score = dp[i-1][j-1] + sim_11[i-1][j-1]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_ptr = (i-1, j-1, 1, 1)\n",
    "\n",
    "            # N:1 merge (ENG block)\n",
    "            for k in range(2, max_merge+1):\n",
    "                if i-k < 0: break\n",
    "                merged_e = merge_embeddings(eng_emb[i-k:i], method)\n",
    "                s = cosine_similarity(merged_e.reshape(1,-1),\n",
    "                                      kor_emb[j-1].reshape(1,-1))[0][0]\n",
    "                score = dp[i-k][j-1] + s + e_bonus\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_ptr = (i-k, j-1, k, 1)\n",
    "\n",
    "            # 1:N merge (KOR block)\n",
    "            for k in range(2, max_merge+1):\n",
    "                if j-k < 0: break\n",
    "                merged_k = merge_embeddings(kor_emb[j-k:j], method)\n",
    "                s = cosine_similarity(eng_emb[i-1].reshape(1,-1),\n",
    "                                      merged_k.reshape(1,-1))[0][0]\n",
    "                score = dp[i-1][j-k] + s + k_bonus\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_ptr = (i-1, j-k, 1, k)\n",
    "\n",
    "            # Skip ENG\n",
    "            score = dp[i-1][j] - skip_penalty\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_ptr = (i-1, j, 1, 0)\n",
    "\n",
    "            # Skip KOR\n",
    "            score = dp[i][j-1] - skip_penalty\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_ptr = (i, j-1, 0, 1)\n",
    "\n",
    "            if best_ptr is None:\n",
    "                best_ptr = (i-1, j-1, 1, 1)\n",
    "\n",
    "            dp[i][j] = best_score\n",
    "            ptr[i][j] = best_ptr\n",
    "\n",
    "    # Back Tranking\n",
    "    aligned = []\n",
    "    unused_eng = set()\n",
    "    unused_kor = set()\n",
    "\n",
    "    i, j = N, M\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        pi, pj, ei, ej = ptr[i][j]\n",
    "\n",
    "        if ei > 0 and ej > 0:  # matched block\n",
    "            aligned.append({\n",
    "                \"eng_idx\": list(range(pi, pi+ei)),\n",
    "                \"kor_idx\": list(range(pj, pj+ej)),\n",
    "                \"eng\": \" \".join(eng_pars[pi:pi+ei]),\n",
    "                \"kor\": \" \".join(kor_pars[pj:pj+ej]),\n",
    "            })\n",
    "        elif ei > 0 and ej == 0:\n",
    "            unused_eng.add(i-1)\n",
    "        elif ei == 0 and ej > 0:\n",
    "            unused_kor.add(j-1)\n",
    "\n",
    "        i, j = pi, pj\n",
    "\n",
    "    aligned = aligned[::-1]\n",
    "    if debug:\n",
    "        print(f\"Unused ENG = {len(unused_eng)}, Unused KOR = {len(unused_kor)}\")\n",
    "\n",
    "    return aligned\n",
    "\n",
    "def align_sentences_from_paragraphs_v2(aligned_paragraphs):\n",
    "    final_pairs = []\n",
    "\n",
    "    for p in aligned_paragraphs:\n",
    "        eng_text = p[\"eng\"]\n",
    "        kor_text = p[\"kor\"]\n",
    "\n",
    "        # 문장 분리\n",
    "        eng_sents = split_sentences(eng_text)\n",
    "        kor_sents = split_sentences(kor_text)\n",
    "\n",
    "        if len(kor_sents) < len(eng_sents): # 영어가 더 많으니까 영어 블락 만드는걸 장려\n",
    "            kor_bonus = 0.06\n",
    "            eng_bonus = 0.12\n",
    "        elif len(kor_sents) > len(eng_sents):\n",
    "            kor_bonus = 0.12\n",
    "            eng_bonus = 0.06\n",
    "        else:\n",
    "            kor_bonus = 0.06\n",
    "            eng_bonus = 0.06\n",
    "\n",
    "\n",
    "        # block DP sentence alignment\n",
    "        sent_align = paragraph_dp_align_v2(\n",
    "            eng_sents, kor_sents,\n",
    "            debug = True,\n",
    "            max_merge=3,       \n",
    "            method=\"mean\",\n",
    "            k_bonus = kor_bonus,\n",
    "            e_bonus = eng_bonus\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        for a in sent_align:\n",
    "            final_pairs.append({\n",
    "                \"eng\": a[\"eng\"],\n",
    "                \"kor\": a[\"kor\"],\n",
    "                \"eng_idx\": a[\"eng_idx\"],\n",
    "                \"kor_idx\": a[\"kor_idx\"],\n",
    "                \"paragraph_eng\": eng_text,\n",
    "                \"paragraph_kor\": kor_text\n",
    "            })\n",
    "\n",
    "    return final_pairs\n",
    "\n",
    "# Embedding for ENG\n",
    "\n",
    "def get_embeddings_flexible(sentences, model_name, debug=True, output_type=\"mean\" ):\n",
    "\n",
    "    print(f\"Load Model: {model_name} | Mode: {output_type}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\" Error : faield to load {model_name} : {e}\")\n",
    "        return None\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # 배치 처리\n",
    "    batch_size = 32\n",
    "    all_results = []\n",
    "    iterator = range(0, len(sentences), batch_size)\n",
    "    if debug:\n",
    "        iterator = tqdm(iterator, desc=f\"  -> Embedding Extract ({model_name})\")\n",
    "    \n",
    "    for i in iterator:\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            hidden_states = outputs.hidden_states  \n",
    "\n",
    "        if output_type == \"mean\":\n",
    "            layer_indices = [8, 10, 12] \n",
    "            pooled_layers = []\n",
    "\n",
    "            for li in layer_indices:\n",
    "                h = hidden_states[li]    \n",
    "                mask_exp = attention_mask.unsqueeze(-1).float()  \n",
    "\n",
    "                sum_h = torch.sum(h * mask_exp, dim=1) \n",
    "                denom = torch.clamp(mask_exp.sum(dim=1), min=1e-9)\n",
    "                mean_h = sum_h / denom        \n",
    "\n",
    "                pooled_layers.append(mean_h)   \n",
    "\n",
    "            sent_vec = torch.cat(pooled_layers, dim=-1) \n",
    "            all_results.append(sent_vec.cpu().numpy())\n",
    "\n",
    "        elif output_type == \"token\":\n",
    "            batch_list = []\n",
    "            for j in range(len(batch_sentences)):\n",
    "                seq_len = attention_mask[j].sum().item()\n",
    "                token_emb = last_hidden_state[j, :seq_len, :].cpu().numpy()\n",
    "                batch_list.append(token_emb)\n",
    "            all_results.extend(batch_list)\n",
    "\n",
    "        else: \n",
    "            cls_emb = last_hidden_state[:, 0, :]\n",
    "            all_results.append(cls_emb.cpu().numpy())\n",
    "\n",
    "    if output_type == \"token\":\n",
    "        return np.array(all_results, dtype=object)\n",
    "    else:\n",
    "        return np.vstack(all_results)\n",
    "\n",
    "\n",
    "# Embedding for Kor\n",
    "def get_kobert_embeddings_flexible(sentences, debug=True, output_type=\"mean\"):\n",
    "\n",
    "    batch_size = 32\n",
    "    all_results = []\n",
    "    iterator = range(0, len(sentences), batch_size)\n",
    "    if debug:\n",
    "        iterator = tqdm(iterator, desc=\"  -> Extracting Embedding with KoBERT\")\n",
    "\n",
    "    for i in iterator:\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        inputs = kobert_tokenizer(\n",
    "            batch_sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = kobert_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "            last_hidden_state = out.last_hidden_state\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            hidden_states = out.hidden_states    \n",
    "\n",
    "        if output_type == \"mean\":\n",
    "            layer_indices = [8, 10, 12]\n",
    "            pooled_layers = []\n",
    "\n",
    "            for li in layer_indices:\n",
    "                h = hidden_states[li] \n",
    "                mask_exp = attention_mask.unsqueeze(-1).float()\n",
    "\n",
    "                sum_h = torch.sum(h * mask_exp, dim=1) \n",
    "                denom = torch.clamp(mask_exp.sum(dim=1), min=1e-9)\n",
    "                mean_h = sum_h / denom\n",
    "\n",
    "                pooled_layers.append(mean_h)\n",
    "\n",
    "            sent_vec = torch.cat(pooled_layers, dim=-1) \n",
    "            all_results.append(sent_vec.cpu().numpy())\n",
    "\n",
    "        elif output_type == \"token\":\n",
    "            batch_list = []\n",
    "            for j in range(len(batch_sentences)):\n",
    "                seq_len = attention_mask[j].sum().item()\n",
    "                token_emb = last_hidden_state[j, :seq_len, :].cpu().numpy()\n",
    "                batch_list.append(token_emb)\n",
    "            all_results.extend(batch_list)\n",
    "            \n",
    "        else: # cls\n",
    "            cls_emb = last_hidden_state[:, 0, :]\n",
    "            all_results.append(cls_emb.cpu().numpy())\n",
    "\n",
    "    if output_type == \"token\":\n",
    "        return np.array(all_results, dtype=object)\n",
    "    else:\n",
    "        return np.vstack(all_results)\n",
    "\n",
    "def get_boundary_tokens(model_type: str):\n",
    "\n",
    "    model_type = model_type.lower()\n",
    "\n",
    "    # SentencePiece 계열 (kobert, xlm-r, electra-kor 등)\n",
    "    if \"kobert\" in model_type or \"xlm\" in model_type or \"sentencepiece\" in model_type:\n",
    "        return \"▁▲\", \"▁△\"\n",
    "\n",
    "    # WordPiece 계열 (bert-base, bert-multilingual, roberta 등)\n",
    "    return \"■\", \"●\"\n",
    "\n",
    "\n",
    "\n",
    "def build_marked_paragraph(paragraph_text, target_idx, model_type):\n",
    "    \"\"\"\n",
    "    wrap target_idx sentence with START/END\n",
    "    Create new marked_paragraph.\n",
    "    \"\"\"\n",
    "\n",
    "    START, END = get_boundary_tokens(model_type)\n",
    "\n",
    "    if isinstance(target_idx, list):\n",
    "        start_i = target_idx[0]\n",
    "        end_i   = target_idx[-1]\n",
    "    else:\n",
    "        start_i = end_i = target_idx\n",
    "\n",
    "    sentences = split_sentences(paragraph_text)\n",
    "    target_chunk = \" \".join(sentences[start_i:end_i+1])\n",
    "\n",
    "    new_sentences = []\n",
    "    new_sentences.extend(sentences[:start_i])              \n",
    "    new_sentences.append(f\"{START} {target_chunk} {END}\")  \n",
    "    new_sentences.extend(sentences[end_i+1:])             \n",
    "\n",
    "    marked_paragraph = \" \".join(new_sentences)\n",
    "    return marked_paragraph\n",
    "\n",
    "def get_contextual_sentence_embedding(\n",
    "    paragraph_text,\n",
    "    target_idx,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_type,\n",
    "    layer_indices=[8, 10, 12]\n",
    "):\n",
    "    \"\"\"\n",
    "    모델 타입(KoBERT/BERT/XLM-R)에 따라 boundary 토큰 자동 선택.\n",
    "    1. target sentence만 boundary로 감싸기\n",
    "    2. tokenize + forward\n",
    "    3. boundary span pooling\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    marked_paragraph = build_marked_paragraph(paragraph_text, target_idx, model_type)\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        marked_paragraph,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
    "\n",
    "    input_ids = encoded[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # boundary 토큰 가져오기\n",
    "    START, END = get_boundary_tokens(model_type)\n",
    "\n",
    "    try:\n",
    "        start_pos = tokens.index(START) + 1\n",
    "        end_pos   = tokens.index(END) - 1\n",
    "    except ValueError:\n",
    "        print(\"boundary span 매칭 실패 -> CLS fallback\")\n",
    "        with torch.no_grad():\n",
    "            out = model(**encoded, output_hidden_states=True)\n",
    "        return out.hidden_states[-1][0, 0].cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = out.hidden_states\n",
    "\n",
    "\n",
    "    pooled = []\n",
    "    for li in layer_indices:\n",
    "        h = hidden_states[li][0]  \n",
    "        vec = h[start_pos:end_pos+1].mean(dim=0)\n",
    "        pooled.append(vec.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(pooled, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd348b46",
   "metadata": {},
   "source": [
    "# 1. Converting and result Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a419bdd",
   "metadata": {},
   "source": [
    "## Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b34a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 차원수가 지금 2304라 이거 각 layer의 mean을 쓰거나, 아니면 나중에 데이터양을 늘리자, 책 2권 더 해서\n",
    "# Setting\n",
    "ENG_FILE_PATH = \"text_source/alice_eng_chapter.txt\"\n",
    "KOR_FILE_PATH = \"text_source/alice_kor_chapter.txt\"\n",
    "PARAGRAPH_FILE_PATH = \"days_results/day1_paragraph_data.npz\"\n",
    "SENTENCE_FILE_PATH = \"days_results/day1_sentence_data.npz\"\n",
    "EDU_FILE_PATH = \"days_results/day1_edu_data.npz\"\n",
    "EMBED_DEBUG = False\n",
    "\n",
    "# Loading 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    eng_raw = load_and_segment_text(ENG_FILE_PATH)\n",
    "    eng_paragraph = split_paragraphs(eng_raw)\n",
    "    eng_sentences = split_sentences(\"\\n\".join(eng_paragraph))\n",
    "\n",
    "    kor_raw = load_and_segment_text(KOR_FILE_PATH)\n",
    "    kor_paragraph = split_paragraphs(kor_raw)\n",
    "    kor_sentences = split_sentences(\"\\n\".join(kor_paragraph))\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "\n",
    "    save_sentences(eng_paragraph, \"text_results/eng_paragraph.txt\")\n",
    "    save_sentences(kor_paragraph, \"text_results/kor_paragraph.txt\")\n",
    "\n",
    "    print(f\"Extract Paragraph: ENG={len(eng_paragraph)}, KOR={len(kor_paragraph)}\")\n",
    "\n",
    "    aligned_paragraph = paragraph_dp_align_v2(eng_paragraph, kor_paragraph, debug = True)\n",
    "    print(f\"Aligned Paragraph Amount: {len(aligned_paragraph)}\")\n",
    "\n",
    "    df = pd.DataFrame(aligned_paragraph)\n",
    "\n",
    "    print(\"Extracting Eng Embedding\")\n",
    "    final_par_eng = get_embeddings_flexible(df[\"eng\"].tolist(), \"bert-base-uncased\",  EMBED_DEBUG)\n",
    "\n",
    "    print(\"Extracting Kor Embedding\")\n",
    "    final_par_kor = get_kobert_embeddings_flexible(df[\"kor\"].tolist(), EMBED_DEBUG)\n",
    "\n",
    "    np.savez(\n",
    "        PARAGRAPH_FILE_PATH,\n",
    "        eng_embs=final_par_eng,\n",
    "        kor_embs=final_par_kor,\n",
    "        eng_sents=df[\"eng\"].values,\n",
    "        kor_sents=df[\"kor\"].values\n",
    "    )\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "# TODO\n",
    "# 문단 자르기 \n",
    "# 문단을 dp 정렬하되, xml-r을 쓰기 함\n",
    "# 그리고 문단마다 페어된 상태로 문장들을 페어 맞추기 (이때도 똑같이 xml-r을 사용한 dp aligment)- 함\n",
    "#---------------------------------------------------------------------------------------#\n",
    "    aligned_sentence = align_sentences_from_paragraphs_v2(aligned_paragraph)\n",
    "    print(f\"Aligned Sentences in aligned pargraph: {len(aligned_sentence)}\")\n",
    "\n",
    "    df_sen = pd.DataFrame(aligned_sentence)\n",
    "    #위에까진 xml-r로 정렬이 됌, 그럼 이제 밑에서 \n",
    "#---------------------------------------------------------------------------------------#\n",
    "    # 이거 하려고 했던게, 원래는 문장 정렬\n",
    "    final_sen_eng = []\n",
    "    final_sen_kor = []\n",
    "\n",
    "    for row in df_sen.itertuples():\n",
    "        paragraph_eng = row.paragraph_eng\n",
    "        paragraph_kor = row.paragraph_kor\n",
    "\n",
    "        eng_vec = get_contextual_sentence_embedding(\n",
    "                paragraph_text= row.paragraph_eng,\n",
    "                target_idx = row.eng_idx,\n",
    "                tokenizer = bert_tokenizer,\n",
    "                model = bert_model,\n",
    "                model_type = \"bert\",\n",
    "        )\n",
    "\n",
    "        kor_vec = get_contextual_sentence_embedding(\n",
    "                paragraph_text= row.paragraph_kor,\n",
    "                target_idx = row.kor_idx,\n",
    "                tokenizer = kobert_tokenizer,\n",
    "                model = kobert_model,\n",
    "                model_type = \"kobert\",\n",
    "        )\n",
    "\n",
    "        final_sen_eng.append(eng_vec)\n",
    "        final_sen_kor.append(kor_vec)\n",
    "\n",
    "    final_sen_eng = np.vstack(final_sen_eng)\n",
    "    final_sen_kor = np.vstack(final_sen_kor)\n",
    "\n",
    "    np.savez(\n",
    "        SENTENCE_FILE_PATH,\n",
    "        eng_embs=final_sen_eng,\n",
    "        kor_embs=final_sen_kor,\n",
    "        eng_sents=df_sen[\"eng\"].values,\n",
    "        kor_sents=df_sen[\"kor\"].values,\n",
    "\n",
    "        paragraph_eng=df_sen[\"paragraph_eng\"].values,\n",
    "        paragraph_kor=df_sen[\"paragraph_kor\"].values,\n",
    "        eng_idx=df_sen[\"eng_idx\"].values,\n",
    "        kor_idx=df_sen[\"kor_idx\"].values\n",
    "    )\n",
    "#---------------------------------------------------------------------------------------#\n",
    "    # This is for without context\n",
    "    print(\"without context Eng Embedding\")\n",
    "    final_sen_eng_without = get_embeddings_flexible(df_sen[\"eng\"].tolist(), \"bert-base-uncased\", EMBED_DEBUG)\n",
    "\n",
    "    print(\"without context Kor Embedding\")\n",
    "    final_sen_kor_without = get_kobert_embeddings_flexible(df_sen[\"kor\"].tolist(), EMBED_DEBUG)\n",
    "  \n",
    "    np.savez(\n",
    "        \"days_results/day1_sentence_data_without.npz\",\n",
    "        eng_embs=final_sen_eng_without,\n",
    "        kor_embs=final_sen_kor_without,\n",
    "        eng_sents=df_sen[\"eng\"].values,\n",
    "        kor_sents=df_sen[\"kor\"].values,\n",
    "        \n",
    "        paragraph_eng=df_sen[\"paragraph_eng\"].values,\n",
    "        paragraph_kor=df_sen[\"paragraph_kor\"].values,\n",
    "        eng_idx=df_sen[\"eng_idx\"].values,\n",
    "        kor_idx=df_sen[\"kor_idx\"].values\n",
    "    )\n",
    "    print(\"Without context sentence npz saved\")\n",
    "#---------------------------------------------------------------------------------------#\n",
    "    print(f\"Finished Saving\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704dbdd",
   "metadata": {},
   "source": [
    "## .npz information checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# File input\n",
    "# example : 'day1_sentence_data_mean.npz' 또는 'day1_sentence_data_token.npz'\n",
    "FILE_NAME = 'days_results/day1_sentence_data.npz' \n",
    "\n",
    "\n",
    "NUM_SAMPLES_TO_SHOW = 5 \n",
    "\n",
    "try:\n",
    "    day1_data = np.load(FILE_NAME, allow_pickle=True)\n",
    "    \n",
    "    print(f\"Checking '{FILE_NAME}' \")\n",
    "    \n",
    "    print(day1_data.files)\n",
    "\n",
    "    # Check data Type\n",
    "    if 'eng_embs' in day1_data:\n",
    "        eng_embs = day1_data['eng_embs']\n",
    "        \n",
    "        is_token_sequence = (eng_embs.dtype == 'O') or (len(eng_embs.shape) == 1)\n",
    "        \n",
    "        print(f\"Aligned Amount: {eng_embs.shape[0]}\")\n",
    "        \n",
    "        if is_token_sequence:\n",
    "            first_shape = eng_embs[0].shape if len(eng_embs) > 0 else \"Unknown\"\n",
    "            print(f\"Embedding Dimension: {first_shape}\")\n",
    "        else:\n",
    "            print(f\"Embedding Dimension: {eng_embs.shape[1]}\")\n",
    "    \n",
    "    # Load Aligned sentences\n",
    "    eng_sents = day1_data['eng_sents']\n",
    "    kor_sents = day1_data['kor_sents']\n",
    "    \n",
    "    print(f\"\\n Aligned Sentences {NUM_SAMPLES_TO_SHOW} List\")\n",
    "    \n",
    "    for i in range(len(eng_sents)):\n",
    "        print(f\"\\n----# {i+1}----\")\n",
    "        print(f\" ENG: {eng_sents[i]}\")\n",
    "        print(f\" KOR: {kor_sents[i]}\")\n",
    "        \n",
    "    day1_data.close()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: cannot find '{FILE_NAME}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc66847",
   "metadata": {},
   "source": [
    "## .npz NaN Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 0. 설정 ---\n",
    "# .npz List you want to check\n",
    "FILES_TO_CHECK = [\n",
    "    \"days_results/day1_sentence_data.npz\",\n",
    "]\n",
    "\n",
    "def check_abnormal_values(arr, name):\n",
    "    if arr.dtype == 'O': # (Option B)\n",
    "        total_elements = sum(x.size for x in arr)\n",
    "        nan_count = sum(np.isnan(x).sum() for x in arr)\n",
    "        inf_count = sum(np.isinf(x).sum() for x in arr)\n",
    "    else: # (Option A)\n",
    "        total_elements = arr.size\n",
    "        nan_count = np.isnan(arr).sum()\n",
    "        inf_count = np.isinf(arr).sum()\n",
    "\n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(f\"Warning\")\n",
    "        print(f\"- NaN: {nan_count}\")\n",
    "        print(f\"- Inf: {inf_count}\")\n",
    "    else:\n",
    "        print(f\"{name}: no NaN/Inf\")\n",
    "\n",
    "def inspect_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error : cannot find the '{file_path}' \")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        print(f\"Analyzing: {file_path}\")\n",
    "        \n",
    "        eng_sents = data['eng_sents'] if 'eng_sents' in data else []\n",
    "        kor_sents = data['kor_sents'] if 'kor_sents' in data else []\n",
    "        print(f\"Total sentences amount: {len(eng_sents)}\")\n",
    "\n",
    "        # 2. NaN/Inf check\n",
    "        if 'eng_embs' in data:\n",
    "            eng_embs = data['eng_embs']\n",
    "            kor_embs = data['kor_embs'] \n",
    "            \n",
    "            # .npz type check\n",
    "            if eng_embs.dtype != 'O' and len(eng_embs.shape) == 2:\n",
    "                print(f\"Data Type: [Option A] Fixed Vector {eng_embs.shape}\")\n",
    "            elif eng_embs.dtype == 'O' or len(eng_embs.shape) == 1:\n",
    "                print(f\"Data Type: [Option B] non Fixed Vector\")\n",
    "            \n",
    "            # Check Abnormal (NaN/Inf)\n",
    "            check_abnormal_values(eng_embs, \"Eng Embedding\")\n",
    "            check_abnormal_values(kor_embs, \"Kor Embedding\")\n",
    "\n",
    "        # 3. Printing out \n",
    "        print(f\"\\n Sample Printing ({len(eng_sents)}):\")\n",
    "        \n",
    "        for i in range(10):\n",
    "            print(f\"\\n[Pair {i+1}]\")\n",
    "            \n",
    "            print(f\"ENG: {eng_sents[i]}\")\n",
    "            print(f\"KOR: {kor_sents[i]}\")\n",
    "            \n",
    "            if 'eng_embs' in locals():\n",
    "                if eng_embs.dtype != 'O': \n",
    "                    if np.isnan(eng_embs[i]).any() or np.isinf(eng_embs[i]).any():\n",
    "                        print(f\"There is a NaN or inf in this sentence !\")\n",
    "                    else:\n",
    "                        norm_val = np.linalg.norm(eng_embs[i])\n",
    "                        print(f\"Pass (Norm: {norm_val:.4f})\")\n",
    "\n",
    "        data.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for f in FILES_TO_CHECK:\n",
    "        inspect_data(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b11077",
   "metadata": {},
   "source": [
    "# 2. Training Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7fbaf",
   "metadata": {},
   "source": [
    "## Adapter Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import copy\n",
    "\n",
    "# Settings\n",
    "\n",
    "SENTENCE_INPUT_FILE = \"days_results/day1_sentence_data.npz\"\n",
    "SENTENCE_OUTPUT_FILE = \"days_results/day2_sentence_results.npz\"\n",
    "\n",
    "SENTENCE_LONG_INPUT_FILE = \"days_results/day1_sentence_data_long.npz\"\n",
    "SENTENCE_LONG_OUTPUT_FILE = \"days_results/day2_sentence_results_long.npz\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBED_DIM = 2304\n",
    "\n",
    "LR = 5e-4\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 40\n",
    "TEMP = 0.07\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "def load_raw_embeddings(path):\n",
    "    print(f\"\\nLoading: {path}\")\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "\n",
    "    eng = data[\"eng_embs\"]\n",
    "    kor = data[\"kor_embs\"]\n",
    "    eng_sents = data[\"eng_sents\"]\n",
    "    kor_sents = data[\"kor_sents\"]\n",
    "    paragraph_eng = data.get(\"paragraph_eng\", None)\n",
    "    paragraph_kor = data.get(\"paragraph_kor\", None)\n",
    "    eng_idx = data.get(\"eng_idx\", None)\n",
    "    kor_idx = data.get(\"kor_idx\", None)\n",
    "\n",
    "    if eng.ndim == 3:\n",
    "        print(\"Token embeddings detected -> mean pooled\")\n",
    "        eng = np.vstack([np.mean(x, axis=0) for x in tqdm(eng)])\n",
    "        kor = np.vstack([np.mean(x, axis=0) for x in tqdm(kor)])\n",
    "    else:\n",
    "        print(\"Sentence embeddings detected\")\n",
    "\n",
    "    eng = np.nan_to_num(eng)\n",
    "    kor = np.nan_to_num(kor)\n",
    "\n",
    "    return (\n",
    "        eng, kor, # sentence embeddings\n",
    "        eng_sents, kor_sents, # sentence text\n",
    "        paragraph_eng, # context paragraph\n",
    "        paragraph_kor,\n",
    "        eng_idx, kor_idx # in-paragraph indices\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_procrustes(eng_raw, kor_raw):\n",
    "    print(\"\\n Computing Procrustes\")\n",
    "\n",
    "    R, _ = orthogonal_procrustes(eng_raw, kor_raw)\n",
    "    aligned = eng_raw @ R\n",
    "\n",
    "    aligned = aligned / (np.linalg.norm(aligned, axis=1, keepdims=True) + 1e-12)\n",
    "    kor_norm = kor_raw / (np.linalg.norm(kor_raw, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    sim = cosine_similarity(aligned, kor_norm)\n",
    "    acc = np.mean(np.argmax(sim, axis=1) == np.arange(len(sim)))\n",
    "    return acc, R, aligned\n",
    "\n",
    "\n",
    "# Model Definitions\n",
    "\n",
    "class ProcrustesModel(nn.Module):\n",
    "    def __init__(self, R):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"R\", torch.tensor(R, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.R  \n",
    "    \n",
    "class LinearMap(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"Linear model initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc1.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"MLP initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc1.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"Residual MLP initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "class L2Loss(nn.Module):\n",
    "    def forward(self, a, b):\n",
    "        return torch.mean((a - b)**2)\n",
    "\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temp=TEMP):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        a = F.normalize(a, dim=1)\n",
    "        b = F.normalize(b, dim=1)\n",
    "        logits = (a @ b.T) / self.temp\n",
    "        labels = torch.arange(len(a), device=logits.device)\n",
    "        return (self.ce(logits, labels) + self.ce(logits.T, labels)) / 2\n",
    "\n",
    "# Training Loop \n",
    "def train(model, eng, kor):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = InfoNCELoss()\n",
    "    # loss_fn = L2Loss()\n",
    "    t_eng = torch.tensor(eng, dtype=torch.float32).to(DEVICE)\n",
    "    t_kor = torch.tensor(kor, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    patience = 0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        opt.zero_grad()\n",
    "        out = model(t_eng)\n",
    "        loss = loss_fn(out, t_kor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_state = model.state_dict()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if (ep + 1) % 40 == 0:\n",
    "            print(f\" Epoch {ep+1}/{EPOCHS} | loss={loss_val:.4f}\")\n",
    "\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def train_with_r1(model, train_eng, train_kor, val_eng, val_kor):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = InfoNCELoss()\n",
    "\n",
    "    t_train_eng = torch.tensor(train_eng, dtype=torch.float32).to(DEVICE)\n",
    "    t_train_kor = torch.tensor(train_kor, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    best_r1 = -1\n",
    "    patience = 0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(EPOCHS):\n",
    "        # Train step\n",
    "        opt.zero_grad()\n",
    "        out = model(t_train_eng)\n",
    "        loss = loss_fn(out, t_train_kor)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Validation (R@1)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            proj = model(torch.tensor(val_eng, dtype=torch.float32).to(DEVICE))\n",
    "            proj = proj.cpu().numpy()\n",
    "            proj = proj / (np.linalg.norm(proj, axis=1, keepdims=True) + 1e-12)\n",
    "            sim = cosine_similarity(proj, val_kor)\n",
    "            r1 = np.mean(np.argmax(sim, axis=1) == np.arange(len(sim)))\n",
    "\n",
    "        # Print log\n",
    "        if (ep + 1) % 20 == 0:\n",
    "            print(f\" Epoch {ep+1}/{EPOCHS} | loss={loss.item():.4f} | R@1={r1:.2%} | best={best_r1:.2%}\")\n",
    "\n",
    "        # Update best model\n",
    "        if r1 > best_r1:\n",
    "            best_r1 = r1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if patience >= PATIENCE:\n",
    "            print(f\"Early stopping (no improvement for {PATIENCE} epochs)\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, eng, kor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        proj = model(torch.tensor(eng, dtype=torch.float32).to(DEVICE))\n",
    "        proj = proj.cpu().numpy()\n",
    "        proj = proj / (np.linalg.norm(proj, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # kor = kor / (np.linalg.norm(kor, axis=1, keepdims=True) + 1e-12)\n",
    "    sim = cosine_similarity(proj, kor)\n",
    "    acc = np.mean(np.argmax(sim, axis=1) == np.arange(len(sim)))\n",
    "\n",
    "    return acc, proj, sim\n",
    "\n",
    "\n",
    "# Main\n",
    "def run_day2(INPUT, OUTPUT):\n",
    "    (\n",
    "    eng_raw,\n",
    "    kor_raw,\n",
    "    eng_sents,\n",
    "    kor_sents,\n",
    "    paragraph_eng,\n",
    "    paragraph_kor,\n",
    "    eng_idx,\n",
    "    kor_idx\n",
    "    ) = load_raw_embeddings(INPUT)\n",
    "\n",
    "\n",
    "    eng_norm = eng_raw / (np.linalg.norm(eng_raw, axis=1, keepdims=True) + 1e-12)\n",
    "    kor_norm = kor_raw / (np.linalg.norm(kor_raw, axis=1, keepdims=True) + 1e-12)\n",
    "    # Split test and train data\n",
    "    N = len(eng_raw)\n",
    "    S = int(N * 0.8)\n",
    "\n",
    "    train_eng, test_eng = eng_norm[:S], eng_norm[S:]\n",
    "    train_kor, test_kor = kor_norm[:S], kor_norm[S:]\n",
    "\n",
    "    procrustes_acc, R, procrustes_proj = compute_procrustes(train_eng, train_kor)\n",
    "\n",
    "    print(\"\\nTRAINING MODELSS\")\n",
    "\n",
    "    models = {\n",
    "        # \"Procrustes\" : ProcrustesModel(R).to(DEVICE),\n",
    "        \"MLP_Pro\": MLP(R_init=R).to(DEVICE),\n",
    "        \"MLP_Rand\": MLP().to(DEVICE),\n",
    "        \"Res_Pro\": ResidualMLP(R_init=R).to(DEVICE),\n",
    "        \"Res_Rand\": ResidualMLP().to(DEVICE),\n",
    "        \"Linear_Pro\": LinearMap(R_init=R).to(DEVICE),\n",
    "        \"Linear_Rand\": LinearMap().to(DEVICE),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}\")\n",
    "        if name == \"Procrustes\":\n",
    "            trained = model\n",
    "        else:\n",
    "            trained = train_with_r1(model, train_eng, train_kor, test_eng,test_kor)\n",
    "        acc, proj, sim = evaluate(trained, test_eng, test_kor)\n",
    "        results[name] = {\"acc\": acc, \"proj\": proj, \"model\": trained}\n",
    "        print(f\"{name} R@1: {acc:.2%}\")\n",
    "\n",
    "    # -------- SELECT BEST NON-PROCRUSTES MODEL --------\n",
    "    print(\"\\nSelecting Best Model\")\n",
    "    best_model_name = max(\n",
    "        [k for k in results.keys() if \"Procrustes\" not in k], \n",
    "        key=lambda x: results[x][\"acc\"]\n",
    "    )\n",
    "    best_model_proj = results[best_model_name][\"proj\"]\n",
    "    best_model_acc = results[best_model_name][\"acc\"]\n",
    "    best_model = results[best_model_name][\"model\"]\n",
    "    # Save best model weights for Day3\n",
    "    torch.save(best_model.state_dict(), OUTPUT.replace(\".npz\", \"_best_model.pt\"))\n",
    "\n",
    "\n",
    "    print(f\"Best model: {best_model_name} (R@1={best_model_acc:.2%})\")\n",
    "    # ---- SAVE RESULTS (FULL FORMAT FOR DAY3) ----\n",
    "    print(\"\\nSaving all results\")\n",
    "\n",
    "    # baseline similarity (basline)\n",
    "    baseline_scores = np.diag(cosine_similarity(test_eng, test_kor))\n",
    "\n",
    "    # aligned similarity (best model)\n",
    "    aligned_scores = np.diag(cosine_similarity(best_model_proj, test_kor))\n",
    "\n",
    "    pro_aligned_scores = np.diag(cosine_similarity(procrustes_proj, train_kor))\n",
    "\n",
    "    np.savez(\n",
    "        OUTPUT,\n",
    "\n",
    "        # --- For Day3 baseline ---\n",
    "        procrustes_proj=procrustes_proj, \n",
    "        procrustes_acc=procrustes_acc,\n",
    "        pro_aligned_scores = pro_aligned_scores,\n",
    "        R = R, \n",
    "\n",
    "        # --- Best model info ---\n",
    "        best_model_name=best_model_name,\n",
    "        best_model_acc=best_model_acc,\n",
    "        projected_eng_embs=best_model_proj,   \n",
    "\n",
    "        # --- For visualization + analysis ---\n",
    "        test_eng_embs=test_eng,\n",
    "        test_kor_embs=test_kor,\n",
    "        baseline_scores=baseline_scores,       \n",
    "        aligned_scores=aligned_scores,         \n",
    "\n",
    "        # --- Sentence metadata ---\n",
    "        test_eng_sents=eng_sents[S:],          \n",
    "        test_kor_sents=kor_sents[S:],\n",
    "\n",
    "        test_paragraph_eng = paragraph_eng[S:],\n",
    "        test_paragraph_kor = paragraph_kor[S:],\n",
    "        test_eng_idx = eng_idx[S:],\n",
    "        test_kor_idx = kor_idx[S:],\n",
    "    )\n",
    "\n",
    "    print(f\"Saved to {OUTPUT}\")\n",
    "\n",
    "run_day2(\"days_results/day1_sentence_data_without.npz\", \"days_results/day2_sentence_results_without.npz\")\n",
    "run_day2(SENTENCE_INPUT_FILE, SENTENCE_OUTPUT_FILE)\n",
    "run_day2(\"days_results/day1_sentence_data_long_without.npz\", \"days_results/day2_sentence_results_long_without.npz\")\n",
    "run_day2(SENTENCE_LONG_INPUT_FILE, SENTENCE_LONG_OUTPUT_FILE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfcec00",
   "metadata": {},
   "source": [
    "# 3. Test the trained Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "EMBED_DIM = 2304\n",
    "EMBED_DEBUG = False   # If True debugging printing on / False debugging print off\n",
    "INPUT = \"days_results/day2_sentence_results.npz\"\n",
    "\n",
    "\n",
    "# 0. Model Definitions\n",
    "\n",
    "class ProcrustesModel(nn.Module):\n",
    "    def __init__(self, R):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"R\", torch.tensor(R, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.R \n",
    "    \n",
    "class LinearMap(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"Linear model initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc1.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"MLP initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, dim=EMBED_DIM, R_init=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "        if R_init is not None:\n",
    "            if R_init.shape == (dim, dim):\n",
    "                self.fc1.weight.data.copy_(torch.from_numpy(R_init.T.astype(np.float32)))\n",
    "                print(\"Residual MLP initialized with Procrustes matrix\")\n",
    "            else:\n",
    "                print(\"Warning: R_init shape mismatch, skipping initialization.\")\n",
    "\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GPT based nuance-shift paraphrase creator \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def gpt_generate_paraphrases(sentence):\n",
    "    prompt = f\"\"\"\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Generate 4 English paraphrases that keep a roughly similar surface meaning,\n",
    "but the nuance, tone, attitude, or emotional framing should be noticeably different.\n",
    "Each paraphrase must feel distinct in nuance, even if the literal meaning is similar.\n",
    "\n",
    "Format:\n",
    "\n",
    "[PARA1]\n",
    "sentence\n",
    "\n",
    "[PARA2]\n",
    "sentence\n",
    "\n",
    "[PARA3]\n",
    "sentence\n",
    "\n",
    "[PARA4]\n",
    "sentence\n",
    "\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    res = completion.choices[0].message.content\n",
    "\n",
    "    paras = []\n",
    "    for tag in [\"[PARA1]\", \"[PARA2]\", \"[PARA3]\", \"[PARA4]\"]:\n",
    "        try:\n",
    "            part = res.split(tag)[1].split(\"[\")[0].strip()\n",
    "            paras.append(part)\n",
    "        except:\n",
    "            paras.append(\"\")\n",
    "\n",
    "    return paras\n",
    "\n",
    "\n",
    "\n",
    "# 1. Sentence Quality checker\n",
    "def perform_nuance_analysis(eng_sents, kor_sents, baseline_scores, aligned_scores, top_k=3):\n",
    "\n",
    "    print(\"(Qualitative Case Study)\")\n",
    "\n",
    "    diff = aligned_scores - baseline_scores\n",
    "\n",
    "    # best improvements\n",
    "    top_idx = np.argsort(diff)[::-1][:top_k]\n",
    "\n",
    "    print(\"\\n Quality increased sentences Top-K\")\n",
    "    for i in top_idx:\n",
    "        print(f\"\\n Increased amount: {diff[i]:.4f}\")\n",
    "        print(\"ENG:\", eng_sents[i])\n",
    "        print(\"KOR:\", kor_sents[i])\n",
    "        print(f\"Before={baseline_scores[i]:.4f} -> After={aligned_scores[i]:.4f}\")\n",
    "\n",
    "    return top_idx\n",
    "\n",
    "\n",
    "def visualize_alignment_arrows(eng_before, kor, eng_after, perplexity=10):\n",
    "    \"\"\"\n",
    "    # Arrow plot showing ENG_before -> ENG_after movement using t-SNE (cosine distance)\n",
    "\n",
    "    eng_before: english embeddings before alignment\n",
    "    eng_after: english embeddings after alignment\n",
    "    kor: korean embeddings \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nRunning t-SNE for Arrow Plot \")\n",
    "\n",
    "    all_data = np.vstack([eng_before, eng_after, kor])\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        metric=\"cosine\",\n",
    "        learning_rate=\"auto\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    all_2d = tsne.fit_transform(all_data)\n",
    "\n",
    "    N = len(eng_before)\n",
    "\n",
    "    eng_before_2d = all_2d[:N]\n",
    "    eng_after_2d  = all_2d[N:2*N]\n",
    "    kor_2d        = all_2d[2*N:3*N]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # before ENG\n",
    "    plt.scatter(eng_before_2d[:,0], eng_before_2d[:,1],\n",
    "                color=\"blue\", alpha=0.7, label=\"ENG_before\")\n",
    "\n",
    "    # after ENG \n",
    "    plt.scatter(eng_after_2d[:,0], eng_after_2d[:,1],\n",
    "                color=\"green\", alpha=0.7, label=\"ENG_after\")\n",
    "\n",
    "    # korean target\n",
    "    plt.scatter(kor_2d[:,0], kor_2d[:,1],\n",
    "                color=\"orange\", alpha=0.7, label=\"KOR\")\n",
    "\n",
    "    # arrows: ENG_before -> ENG_after\n",
    "    for i in range(N):\n",
    "        plt.arrow(\n",
    "            eng_before_2d[i,0], eng_before_2d[i,1],\n",
    "            eng_after_2d[i,0] - eng_before_2d[i,0],\n",
    "            eng_after_2d[i,1] - eng_before_2d[i,1],\n",
    "            color=\"gray\",\n",
    "            alpha=0.5,\n",
    "            width=0.05,\n",
    "            length_includes_head=True,\n",
    "            head_width=1.0\n",
    "        )\n",
    "\n",
    "    plt.title(\"ENG Alignment Path (Arrow Plot via t-SNE)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# run nuance test for no ctx npz. \n",
    "def run_nuance_test_without(eng_sents, kor_sents, best_model, indices):\n",
    "\n",
    "    print(\"Nuance Perturbation Test\")\n",
    "\n",
    "    for idx in indices:\n",
    "        src = eng_sents[idx]\n",
    "        tgt = kor_sents[idx]\n",
    "\n",
    "        print(\"ENG :\", src)\n",
    "        print(\"KOR :\", tgt)\n",
    "\n",
    "        # ENG embedding\n",
    "        src_emb = get_embeddings_flexible([src], \"bert-base-uncased\", EMBED_DEBUG)\n",
    "        tgt_emb = get_kobert_embeddings_flexible([tgt], EMBED_DEBUG)\n",
    "\n",
    "        src_emb = src_emb / (np.linalg.norm(src_emb, axis=1, keepdims=True) + 1e-12)\n",
    "        tgt_emb = tgt_emb / (np.linalg.norm(tgt_emb, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "        # Best model forward \n",
    "        with torch.no_grad():\n",
    "            proj_src = best_model(torch.tensor(src_emb, dtype=torch.float32).to(DEVICE))\n",
    "        proj_src = proj_src.cpu().numpy()\n",
    "        proj_src = proj_src / (np.linalg.norm(proj_src, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "        sim_orig = cosine_similarity(proj_src, tgt_emb)[0][0]\n",
    "        print(f\"원문 sim = {sim_orig:.4f}\")\n",
    "\n",
    "        # GPT paraphrase\n",
    "        paras = gpt_generate_paraphrases(src)\n",
    "\n",
    "        for j, p in enumerate(paras, start=1):\n",
    "            emb_p = get_embeddings_flexible([p], \"bert-base-uncased\", EMBED_DEBUG)\n",
    "            emb_p = emb_p / (np.linalg.norm(emb_p, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                proj_p = best_model(torch.tensor(emb_p, dtype=torch.float32).to(DEVICE))\n",
    "            proj_p = proj_p.cpu().numpy()\n",
    "            proj_p = proj_p / (np.linalg.norm(proj_p, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "            sim_p = cosine_similarity(proj_p, tgt_emb)[0][0]\n",
    "\n",
    "            print(f\"\\nPARA{j}: sim={sim_p:.4f}\")\n",
    "            print(\"->\", p)\n",
    "\n",
    "# function to repleac the idx sentence to GPT PARA\n",
    "def get_contextual_embedding_with_replacement(\n",
    "    paragraph_text,\n",
    "    target_idx,\n",
    "    new_sentence,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    model_type,\n",
    "    layer_indices=[8, 10, 12]\n",
    "):\n",
    "\n",
    "    sentences = split_sentences(paragraph_text)\n",
    "\n",
    "    # target idx arrange\n",
    "    if isinstance(target_idx, list):\n",
    "        start_i = target_idx[0]\n",
    "        end_i   = target_idx[-1]\n",
    "    else:\n",
    "        start_i = end_i = target_idx\n",
    "\n",
    "    if start_i < 0 or end_i >= len(sentences):\n",
    "        raise ValueError(\"invalid target_idx\")\n",
    "\n",
    "    # target span sentence replacement\n",
    "    new_sent_list = sentences[:start_i] + [new_sentence] + sentences[end_i+1:]\n",
    "\n",
    "    modified_paragraph = \" \".join(new_sent_list)\n",
    "\n",
    "    return get_contextual_sentence_embedding(\n",
    "        paragraph_text = modified_paragraph,\n",
    "        target_idx = start_i,        \n",
    "        tokenizer = tokenizer,\n",
    "        model = model,\n",
    "        model_type = model_type,\n",
    "        layer_indices = layer_indices\n",
    "    )\n",
    "\n",
    "\n",
    "def run_nuance_test(\n",
    "    eng_sents,\n",
    "    kor_sents,\n",
    "    paragraph_eng,\n",
    "    paragraph_kor,\n",
    "    eng_idx,\n",
    "    kor_idx,\n",
    "    best_model,\n",
    "    indices\n",
    "):\n",
    "\n",
    "    print(\"Nuance Perturbation Test (Contextual)\")\n",
    "\n",
    "    for idx in indices:\n",
    "        src = eng_sents[idx]\n",
    "        tgt = kor_sents[idx]\n",
    "\n",
    "        print(\"ENG :\", src)\n",
    "        print(\"ENG paragraph:\", paragraph_eng[idx])\n",
    "        print(\"KOR :\", tgt)\n",
    "        print(\"KOR paragraph:\", paragraph_kor[idx])\n",
    "\n",
    "        # ============================\n",
    "        # 1) 원문 contextual embedding\n",
    "        # ============================\n",
    "        src_emb = get_contextual_sentence_embedding(\n",
    "            paragraph_text = paragraph_eng[idx],\n",
    "            target_idx = eng_idx[idx],\n",
    "            tokenizer = bert_tokenizer,\n",
    "            model = bert_model,\n",
    "            model_type = \"bert\",\n",
    "        )\n",
    "\n",
    "        tgt_emb = get_contextual_sentence_embedding(\n",
    "            paragraph_text = paragraph_kor[idx],\n",
    "            target_idx = kor_idx[idx],\n",
    "            tokenizer = kobert_tokenizer,\n",
    "            model = kobert_model,\n",
    "            model_type = \"kobert\",\n",
    "        )\n",
    "\n",
    "        # normalize\n",
    "        src_emb = src_emb.reshape(1, -1)\n",
    "        tgt_emb = tgt_emb.reshape(1, -1)\n",
    "\n",
    "        src_emb /= (np.linalg.norm(src_emb, axis=1, keepdims=True) + 1e-12)\n",
    "        tgt_emb /= (np.linalg.norm(tgt_emb, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "        #  Model forward \n",
    "        with torch.no_grad():\n",
    "            proj_src = best_model(torch.tensor(src_emb, dtype=torch.float32).to(DEVICE))\n",
    "        proj_src = proj_src.cpu().numpy()\n",
    "        proj_src /= (np.linalg.norm(proj_src, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "        sim_orig = cosine_similarity(proj_src, tgt_emb)[0][0]\n",
    "        print(f\"원문 sim = {sim_orig:.4f}\")\n",
    "\n",
    "\n",
    "        # 2) 4 GPT paraphrase \n",
    "        paras = gpt_generate_paraphrases(src)\n",
    "\n",
    "        for j, p in enumerate(paras, start=1):\n",
    "\n",
    "            emb_p = get_contextual_embedding_with_replacement(\n",
    "                paragraph_text = paragraph_eng[idx],\n",
    "                target_idx = eng_idx[idx],\n",
    "                new_sentence = p,\n",
    "                tokenizer = bert_tokenizer,\n",
    "                model = bert_model,\n",
    "                model_type = \"bert\"\n",
    "            )\n",
    "\n",
    "            emb_p = emb_p.reshape(1, -1)\n",
    "            emb_p /= (np.linalg.norm(emb_p, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                proj_p = best_model(torch.tensor(emb_p, dtype=torch.float32).to(DEVICE))\n",
    "            proj_p = proj_p.cpu().numpy()\n",
    "            proj_p /= (np.linalg.norm(proj_p, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "            sim_p = cosine_similarity(proj_p, tgt_emb)[0][0]\n",
    "\n",
    "            print(f\"\\nPARA{j}: sim={sim_p:.4f}\")\n",
    "            print(\"->\", p)\n",
    "\n",
    "\n",
    "# 4. Main : Load Saved Outputs & Run Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Trained Adapter Tester Loading\")\n",
    "\n",
    "\n",
    "    # # Sentence npz data\n",
    "    # data = np.load(\"days_results/day2_sentence_results.npz\", allow_pickle=True)\n",
    "\n",
    "    # Paragraph npz data\n",
    "    data = np.load(INPUT, allow_pickle = True)\n",
    "\n",
    "\n",
    "    # ----- Load according to Day2 keys -----\n",
    "    procrustes_proj  = data[\"procrustes_proj\"]\n",
    "    procrustes_acc   = float(data[\"procrustes_acc\"])\n",
    "    R = data[\"R\"]\n",
    "\n",
    "    best_name        = str(data[\"best_model_name\"])\n",
    "    best_acc         = float(data[\"best_model_acc\"])\n",
    "    proj_eng_embs    = data[\"projected_eng_embs\"]    # aligned ENG embeddings\n",
    "\n",
    "    test_eng_embs    = data[\"test_eng_embs\"]\n",
    "    test_kor_embs    = data[\"test_kor_embs\"]\n",
    "\n",
    "    baseline_scores  = data[\"baseline_scores\"]\n",
    "    aligned_scores   = data[\"aligned_scores\"]\n",
    "\n",
    "    test_eng_sents        = data[\"test_eng_sents\"]\n",
    "    test_kor_sents        = data[\"test_kor_sents\"]\n",
    "\n",
    "    paragraph_eng       = data[\"test_paragraph_eng\"]\n",
    "    paragraph_kor       = data[\"test_paragraph_kor\"]\n",
    "    eng_idx         = data[\"test_eng_idx\"]\n",
    "    kor_idx         = data[\"test_kor_idx\"]\n",
    "\n",
    "    print(f\"Loaded best model = {best_name} (acc={best_acc:.2%})\")\n",
    "\n",
    "    if best_name == \"MLP_Pro\" or best_name == \"MLP_Rand\":\n",
    "        best_model = MLP().to(DEVICE)\n",
    "    elif best_name == \"Res_Pro\" or best_name == \"Res_Rand\":\n",
    "        best_model = ResidualMLP().to(DEVICE)\n",
    "    elif best_name == \"Linear_Pro\" or best_name == \"Linear_Rand\":\n",
    "        best_model = LinearMap().to(DEVICE)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type\")\n",
    "\n",
    "    # weight load\n",
    "    best_model.load_state_dict(\n",
    "        torch.load(INPUT.replace(\".npz\",\"_best_model.pt\"), map_location=DEVICE)\n",
    "    )\n",
    "\n",
    "    best_model.eval()\n",
    "\n",
    "\n",
    "    # 1. Quality Test\n",
    "    top_indices_local = perform_nuance_analysis(\n",
    "        test_eng_sents, test_kor_sents,\n",
    "        baseline_scores,\n",
    "        aligned_scores,\n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    # 2. Create arrow plot \n",
    "    visualize_alignment_arrows(test_eng_embs, test_kor_embs, proj_eng_embs)\n",
    "\n",
    "\n",
    "    # 3. Nuance Perturbation Test\n",
    "\n",
    "    # run_nuance_test_without(eng_sents, kor_sents, best_model, top_indices)\n",
    "    run_nuance_test(\n",
    "        eng_sents = test_eng_sents,\n",
    "        kor_sents = test_kor_sents,\n",
    "        paragraph_eng = paragraph_eng,\n",
    "        paragraph_kor = paragraph_kor,\n",
    "        eng_idx = eng_idx,\n",
    "        kor_idx = kor_idx,\n",
    "        best_model = best_model,\n",
    "        indices = top_indices_local\n",
    "    )\n",
    "\n",
    "    print(\"\\n Finished Printing Trained Adapter Result \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
